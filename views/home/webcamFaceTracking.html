<!DOCTYPE html>
<html>
<head>
  <script src="/dist/face-api.js"></script>
  <script src="/js/commons.js"></script>
  <script src="/js/drawing.js"></script>
  <script src="/js/faceDetectionControls.js"></script>
  <script src="/js/imageSelectionControls.js"></script>
  <script src="/js/bbt.js"></script>
  <link rel="stylesheet" href="/css/styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css">
  <script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js"></script>

</head>
<body>
  <div id="navbar"></div>
  <div class="center-content page-container">
      <div class="">
          <div class="col-md-2">
            <div style="position: relative" class="margin">
              <video onplay="onPlay(this)" id="inputVideo" autoplay muted></video>
              <canvas id="overlay" />
            </div>
          </div>
          <div class="col-md-2 col-md-offset-6">
            <div style="position: relative" class="margin">
              <img id="cropFaceImg" src="" />
              <canvas id="cropFace" />
            </div>
          </div>
        </div>
      </div>
  </div>

  <script>
    let faceMatcher = null
    let forwardTimes = []
    let withFaceLandmarks = false
    let withBoxes = true

    async function updateResults() {
      const videoEl = $('#inputVideo').get(0)

      if(videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded())
        return setTimeout(() => onPlay())

      const options = getFaceDetectorOptions()
      const results = await faceapi.detectAllFaces(videoEl, options).withFaceLandmarks().withFaceDescriptors()
      const canvas = $('#overlay').get(0)

      resizedResults = resizeCanvasAndResults($('#inputVideo').get(0), canvas, results)

      const boxesWithText = resizedResults.map(({ detection, descriptor }) =>
        new faceapi.BoxWithText(
          detection.box,
          faceMatcher.findBestMatch(descriptor).toString()
        )
      )
      faceapi.drawDetection(canvas, boxesWithText)

      const canvases = await faceapi.extractFaces(videoEl, boxesWithText)
      console.log(canvases[0] instanceof HTMLCanvasElement)

      var dataURL = canvases[0].toDataURL()
      document.getElementById('cropFaceImg').src = dataURL
    } 


    function onChangeWithFaceLandmarks(e) {
      withFaceLandmarks = !$(e.target).prop('checked')
    }

    function onChangeHideBoundingBoxes(e) {
      withBoxes = !$(e.target).prop('checked')
    }

    function updateTimeStats(timeInMs) {
      forwardTimes = [timeInMs].concat(forwardTimes).slice(0, 30)
      const avgTimeInMs = forwardTimes.reduce((total, t) => total + t) / forwardTimes.length
      $('#time').val(`${Math.round(avgTimeInMs)} ms`)
      $('#fps').val(`${faceapi.round(1000 / avgTimeInMs)}`)
    }

    async function onPlay() {
      
      updateResults()

      setTimeout(() => onPlay())
    }

    async function run() {
      // load face detection and face landmark models
      await changeFaceDetector(TINY_FACE_DETECTOR)
      await faceapi.loadFaceLandmarkModel('/')
      await faceapi.loadFaceRecognitionModel('/')
      changeInputSize(128)

      // try to access users webcam and stream the images
      // to the video element
      const stream = await navigator.mediaDevices.getUserMedia({ video: {} })
      const videoEl = $('#inputVideo').get(0)
      videoEl.srcObject = stream

      // initialize face matcher with 1 reference descriptor per bbt character
      faceMatcher = await createBbtFaceMatcher(1)
      console.log(faceMatcher)
      updateResults() 

    }

    //function updateResults() {}

    $(document).ready(function() {
      renderNavBar('#navbar', 'webcam_face_tracking')
      //initImageSelectionControls()
      initFaceDetectionControls()
      run()
    })
  </script>
</body>
</html>